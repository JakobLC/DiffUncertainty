"""Create scanner-balanced cross-validation splits for the Chaksu dataset.

This script now consumes the metadata generated by ``process_chaksu.py``
instead of crawling the raw dataset directories. The metadata already captures
the processed sample identifiers together with the split (train/test) and
scanner (machine), which makes split creation reproducible even if the raw
dataset is no longer available. All paths are derived from a single
``dataset_name`` argument (default: ``chaksu128``), assuming the processed
artifacts live under ``values_datasets/<dataset_name>/preprocessed`` and the
splits are written to ``values_datasets/<dataset_name>/splits/...``.

Processing summary:

* Remidio train samples are split into ``num_splits`` folds. Each fold uses
    4/5 of the data for training and 1/5 for validation.
* Remidio test samples form the ``id_test`` set.
* All Bosch and Forus samples (train or test) constitute the ``ood_test`` set.
* Unlabeled pools remain empty because we only need deterministic partitions.

The resulting ``splits.pkl`` mirrors the structure used elsewhere in the
repository (``train``, ``val``, ``id_test``, ``ood_test``,
``id_unlabeled_pool``, ``ood_unlabeled_pool``) and references the processed
dataset via relative paths of the form ``images/<image_file>``.
"""

from __future__ import annotations

import argparse
import csv
import logging
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Sequence

import numpy as np
from sklearn.model_selection import KFold

REMIDIO = "Remidio"
OOD_MACHINES = ("Bosch", "Forus")
DATASETS_ROOT = Path("/home/jloch/Desktop/diff/luzern/values_datasets")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Create scanner-based cross-validation splits for Chaksu",
    )
    parser.add_argument(
        "--dataset-name",
        type=str,
        default="chaksu128",
        help="Name of the processed dataset variant to use",
    )
    parser.add_argument(
        "--num-splits",
        type=int,
        default=5,
        help="Number of folds for Remidio cross-validation",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=123,
        help="Random seed used for shuffling the folds",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Increase logging verbosity",
    )
    return parser.parse_args()


def setup_logging(verbose: bool) -> None:
    logging.basicConfig(
        level=logging.DEBUG if verbose else logging.INFO,
        format="[%(asctime)s] %(levelname)s:%(name)s:%(message)s",
    )


def load_metadata(metadata_path: Path) -> List[Dict[str, str]]:
    if not metadata_path.is_file():
        raise FileNotFoundError(f"Metadata file not found: {metadata_path}")
    with metadata_path.open() as handle:
        reader = csv.DictReader(handle)
        required = {"split", "machine", "image_file"}
        if reader.fieldnames is None or not required.issubset(reader.fieldnames):
            missing = required - set(reader.fieldnames or [])
            raise ValueError(
                f"Metadata file missing required columns: {sorted(missing)}",
            )
        rows = [row for row in reader]
    if not rows:
        raise ValueError("Metadata file is empty; cannot generate splits")
    return rows


def relative_image_path(row: Dict[str, str]) -> str:
    return f"images/{row['image_file']}"


def collect_from_metadata(
    rows: Sequence[Dict[str, str]],
    *,
    split_name: Optional[str] = None,
    machines: Optional[Sequence[str]] = None,
) -> List[str]:
    results: List[str] = []
    for row in rows:
        if split_name is not None and row["split"] != split_name:
            continue
        if machines is not None and row["machine"] not in machines:
            continue
        results.append(relative_image_path(row))
    return sorted(results)


def ensure_output_path(output_path: Path) -> Path:
    if output_path.suffix != ".pkl":
        output_path = output_path / "splits.pkl"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    return output_path


def build_folds(remidio_samples: Sequence[str], num_splits: int, seed: int) -> List[dict]:
    if len(remidio_samples) < num_splits:
        raise ValueError(
            "Number of splits cannot exceed the number of Remidio train samples",
        )
    remidio_array = np.array(remidio_samples)
    kfold = KFold(n_splits=num_splits, shuffle=True, random_state=seed)
    folds: List[dict] = []
    for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(remidio_array)):
        train_split = remidio_array[train_idx]
        val_split = remidio_array[val_idx]
        logging.info(
            "Fold %d: %d train / %d val samples",
            fold_idx,
            len(train_split),
            len(val_split),
        )
        folds.append(
            {
                "train": train_split,
                "val": val_split,
            }
        )
    return folds


def attach_common_sets(
    folds: List[dict],
    id_test: Sequence[str],
    ood_test: Sequence[str],
) -> None:
    id_test_arr = np.array(id_test)
    ood_test_arr = np.array(ood_test)
    empty = np.array([], dtype=object)
    for fold in folds:
        fold["id_test"] = id_test_arr
        fold["ood_test"] = ood_test_arr
        fold["id_unlabeled_pool"] = empty
        fold["ood_unlabeled_pool"] = empty


def main() -> None:
    args = parse_args()
    setup_logging(args.verbose)

    dataset_root = (DATASETS_ROOT / args.dataset_name).expanduser().resolve()
    processed_root = dataset_root / "preprocessed"
    if not processed_root.is_dir():
        raise FileNotFoundError(
            f"Processed dataset directory not found: {processed_root}. Did you run process_chaksu.py?",
        )
    metadata_path = processed_root / "metadata.csv"
    output_path = dataset_root / "splits" / "scanner" / "firstCycle" / "splits.pkl"
    output_path = ensure_output_path(output_path)

    rows = load_metadata(metadata_path)
    remidio_train = collect_from_metadata(rows, split_name="train", machines=[REMIDIO])
    if not remidio_train:
        raise RuntimeError(
            "No Remidio train samples found in metadata. Did you run process_chaksu.py?",
        )

    remidio_test = collect_from_metadata(rows, split_name="test", machines=[REMIDIO])
    if not remidio_test:
        raise RuntimeError(
            "No Remidio test samples found in metadata. Expected split='test' entries",
        )

    ood_test = collect_from_metadata(rows, machines=OOD_MACHINES)
    if not ood_test:
        logging.warning("No Bosch/Forus samples found; OoD test split will be empty")

    folds = build_folds(remidio_train, args.num_splits, args.seed)
    attach_common_sets(folds, remidio_test, ood_test)

    with output_path.open("wb") as handle:
        pickle.dump(folds, handle)
    logging.info("Saved %d folds to %s", len(folds), output_path)


if __name__ == "__main__":
    main()
